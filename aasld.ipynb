{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11952aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "def clean(author):\n",
    "    \n",
    "    new = re.sub('\\n|\\t|\\r', '', author)\n",
    "    new = new = re.sub('\\s{2,}', ' ', new)\n",
    "    return new\n",
    "\n",
    "\n",
    "def sibling_cleaner(sibling_list):\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(sibling_list):\n",
    "        \n",
    "        if sibling_list[i] == '\\n':\n",
    "            del sibling_list[i]\n",
    "            i -= 1\n",
    "            \n",
    "        i += 1\n",
    "    return sibling_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a311e128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neeche ye rha\n",
    "dic = {'urls':[]}\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.aasld.org/the-liver-meeting/schedule-and-speakers')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09072da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "page = 0\n",
    "flag = True\n",
    "while flag:\n",
    "    print(f'page no . :- {page}=============================================\\n')\n",
    "    \n",
    "    time.sleep(6)\n",
    "    home_page = driver.page_source\n",
    "    homesoup = BeautifulSoup(home_page, 'lxml')\n",
    "    \n",
    "\n",
    "    for hpg in homesoup.find_all(class_='info-grid__box'):\n",
    "        if hpg.find(class_='info-grid__title').get('href'):\n",
    "            session_link = 'https://www.aasld.org' + hpg.find(class_='info-grid__title').get('href')\n",
    "            print(session_link)\n",
    "            dic['urls'].append(session_link)\n",
    "\n",
    "    # find next button\n",
    "    \n",
    "    for cl in driver.find_elements(By.TAG_NAME, 'button'):\n",
    "        if cl.text == 'Next' and cl.is_enabled():\n",
    "            found = True\n",
    "            cl.click()\n",
    "            time.sleep(7)\n",
    "        elif cl.text == 'Next' and cl.is_enabled() == False:\n",
    "            flag = False\n",
    "    \n",
    "    page += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4ad7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {\n",
    "    'source_id':[],\n",
    "    'manual_id':[],\n",
    "    'article_title':[],\n",
    "    'url':[],\n",
    "    'authors':[],\n",
    "    'author_affiliation':[],\n",
    "    'abstract_text':[],\n",
    "    'date':[],\n",
    "    'start_time':[],\n",
    "    'end_time':[],\n",
    "    'location':[],\n",
    "    'session_id':[],\n",
    "    'news_type':[],\n",
    "    'session_title':[],\n",
    "    'session_type':[],\n",
    "    'category':[],\n",
    "    'sub_category':[],\n",
    "    'disclosure':[]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7602dead",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c = 0\n",
    "se_no = 1\n",
    "for ou in df['urls']:\n",
    "    \n",
    "    session_id = f'S{se_no}'\n",
    "    print(ou)\n",
    "    req = requests.get(ou)\n",
    "    soup = BeautifulSoup(req.content, 'lxml')\n",
    "\n",
    "    session_title = soup.find(class_='col-12 col-lg-7').find('h3').text.strip() # session title\n",
    "    presentations = []\n",
    "    # session date time and location\n",
    "    session_date = ''\n",
    "    session_time = ''\n",
    "    session_location = ''\n",
    "\n",
    "    date_time_room = clean(soup.find(class_='col-12 col-lg-7').find('p').text)\n",
    "\n",
    "    if re.search('Date.*', date_time_room, flags=re.S):\n",
    "        session_date = re.search('Date:(.*,.*\\d[A-Za-z]{1,2})', date_time_room, flags=re.S).group(1).strip()\n",
    "\n",
    "    if re.search('Time:.*', date_time_room, flags=re.S):\n",
    "        session_time = re.search('Time:(.\\d{1,2}:\\d{1,2}.*- \\d{1,2}:\\d{1,2} [A-Z]{1,2})',date_time_room, flags=re.S).group(1).strip()\n",
    "\n",
    "    if re.search('Room:.*', date_time_room, flags=re.S):\n",
    "        session_location = re.search('Room:(.*)', date_time_room, flags=re.S).group(1).strip()\n",
    "\n",
    "    # abstract text\n",
    "    session_text = ''\n",
    "    for i in sibling_cleaner(list(soup.find(class_='col-12 col-lg-7').find('p').next_siblings)):\n",
    "\n",
    "        ul_text = ''\n",
    "        if i.name == 'ul':\n",
    "            for li in i.find_all('li'):\n",
    "                ul_text += li.text + ' '\n",
    "            if ul_text:\n",
    "                session_text += ul_text + ' '\n",
    "\n",
    "        else:\n",
    "            session_text += i.text + ' '\n",
    "            \n",
    "        #check for presentations...\n",
    "        for pre in i.find_all('a'):\n",
    "            if 'http' not in str(pre.get('href')):\n",
    "                presentations.append(pre.get('href'))\n",
    "\n",
    "\n",
    "    # session author\n",
    "    session_author = ''\n",
    "    session_aff = ''\n",
    "    if soup.find(class_='cta-block__content'):\n",
    "        for i in soup.find(class_='cta-block__content').find_all('a'):\n",
    "            given = i.text.strip()\n",
    "            to_find = str(i.get('href'))\n",
    "\n",
    "            sten = to_find.split('-')[-1][0].upper() + to_find.split('-')[-1][1:-1] + to_find.split('-')[-1][-1].upper()\n",
    "            if len(given.split(to_find.split('-')[-1].upper())) > 1:\n",
    "                session_author += given.split(to_find.split('-')[-1].upper())[0].strip()[:-1] + ' '+ to_find.split('-')[-1].upper() + '; '\n",
    "                session_aff += given.split(to_find.split('-')[-1].upper())[1].strip()[1:] + '; '\n",
    "\n",
    "\n",
    "\n",
    "            elif len(given.split(to_find.split('-')[-1].capitalize())) > 1:\n",
    "\n",
    "                session_author += given.split(to_find.split('-')[-1].capitalize())[0].strip()[:-1] + ' '+ to_find.split('-')[-1].capitalize() + '; '\n",
    "                session_aff += given.split(to_find.split('-')[-1].capitalize())[1].strip()[1:] + '; '\n",
    "\n",
    "\n",
    "            elif len(given.split(to_find.split('-')[-1])) > 1:\n",
    "\n",
    "                session_author += given.split(to_find.split('-')[-1])[0].strip()[:-1] + ' ' + to_find.split('-')[-1] + '; '\n",
    "                session_aff += given.split(to_find.split('-')[-1])[1].strip()[1:] + '; '\n",
    "\n",
    "            elif len(given.split(sten)) > 1:\n",
    "                session_author += given.split(sten)[0].strip()[:-1] + ' ' + sten + '; '\n",
    "                session_aff += given.split(sten)[1].strip()[1:] + '; '\n",
    "\n",
    "    if presentations:\n",
    "        session_text = re.sub('Abstracts.*', '', session_text, flags=re.S)\n",
    "        \n",
    "    if len(session_time.split('-')[0].split()) == 1:\n",
    "        session_time = session_time.split('-')[0].split()[0] + ' ' + session_time.split('-')[1].split()[1] + ' - ' + session_time.split('-')[1].split()[0] + ' ' + session_time.split('-')[1].split()[1]\n",
    "\n",
    "    \n",
    "    print(f'sess title :- {session_title}')\n",
    "    print(f'sess date :- {session_date}')\n",
    "    print(f'sess time :- {session_time}')\n",
    "    print(f'sess loc :- {session_location}')\n",
    "    print(f'sess author :- {session_author}')\n",
    "    print(f'sess aff :- {session_aff}')\n",
    "    print(f'sess text :- {session_text}')\n",
    "\n",
    "    print('========='*13, c)\n",
    "\n",
    "    dic['session_id'].append(session_id)\n",
    "    dic['news_type'].append('Session')\n",
    "    dic['source_id'].append('')\n",
    "    dic['manual_id'].append('')\n",
    "    dic['article_title'].append(session_title)\n",
    "    dic['url'].append(ou)\n",
    "    dic['authors'].append(session_author)\n",
    "    dic['author_affiliation'].append(session_aff)\n",
    "    dic['abstract_text'].append(session_text)\n",
    "    dic['date'].append(session_date)\n",
    "    dic['start_time'].append(session_time)\n",
    "    dic['end_time'].append('')\n",
    "    dic['location'].append(session_location)\n",
    "    dic['session_title'].append(session_title)\n",
    "    dic['session_type'].append('')\n",
    "    dic['category'].append('')\n",
    "    dic['sub_category'].append('')\n",
    "    dic['disclosure'].append('')\n",
    "    if presentations:\n",
    "        print(session_link)\n",
    "        try:\n",
    "        \n",
    "            session_title = df_data.iloc[c]['session_title']\n",
    "            session_location = df_data.iloc[c]['location']\n",
    "            session_type = df_data.iloc[c]['session_type']\n",
    "            session_category = df_data.iloc[c]['category']\n",
    "            sub_cat = df_data.iloc[c]['sub_category']\n",
    "            session_date = df_data.iloc[c]['date']\n",
    "            print(session_title)\n",
    "\n",
    "\n",
    "            for p in presentations:\n",
    "                abstract_link = f'https://www.aasld.org{p}'\n",
    "                print(abstract_link)\n",
    "                print(c)\n",
    "\n",
    "                req = requests.get(abstract_link)\n",
    "                soup = BeautifulSoup(req.content, 'lxml')\n",
    "                article_title = soup.find(class_='col-12 col-lg-7').find('h3').text.strip() # session title\n",
    "\n",
    "                article_time = ''\n",
    "                date_time_room = clean(soup.find(class_='cta-block__content').text)\n",
    "\n",
    "                if re.search('Time:.*', date_time_room, flags=re.S):\n",
    "                    article_time = re.search('Time:(.\\d{1,2}:\\d{1,2}.*- \\d{1,2}:\\d{1,2} [A-Z]{1,2})',date_time_room, flags=re.S).group(1).strip()\n",
    "\n",
    "\n",
    "                # abstract text\n",
    "                article_text = ''\n",
    "                new_text = [soup.find(class_='col-12 col-lg-7').find('p')] + list(soup.find(class_='col-12 col-lg-7').find('p').next_siblings)\n",
    "                for i in sibling_cleaner(new_text):\n",
    "\n",
    "                    ul_text = ''\n",
    "                    if i.name == 'ul':\n",
    "                        for li in i.find_all('li'):\n",
    "                            ul_text += li.text + ' '\n",
    "                        if ul_text:\n",
    "                            article_text += ul_text + ' '\n",
    "\n",
    "                    else:\n",
    "                        article_text += i.text + ' '\n",
    "\n",
    "                # session author\n",
    "                article_author = ''\n",
    "                article_aff = ''\n",
    "                if soup.find(class_='cta-block__content'):\n",
    "                    for i in soup.find(class_='cta-block__content').find_all('a')[:-1]:\n",
    "                        given = i.text.strip()\n",
    "                        to_find = str(i.get('href'))\n",
    "\n",
    "                        sten = to_find.split('-')[-1][0].upper() + to_find.split('-')[-1][1:-1] + to_find.split('-')[-1][-1].upper()\n",
    "                        if len(given.split(to_find.split('-')[-1].upper())) > 1:\n",
    "                            article_author += given.split(to_find.split('-')[-1].upper())[0].strip()[:-1] + ' '+ to_find.split('-')[-1].upper() + '; '\n",
    "                            article_aff += given.split(to_find.split('-')[-1].upper())[1].strip()[1:] + '; '\n",
    "\n",
    "\n",
    "                        elif len(given.split(to_find.split('-')[-1].capitalize())) > 1:\n",
    "\n",
    "                            article_author += given.split(to_find.split('-')[-1].capitalize())[0].strip()[:-1] + ' '+ to_find.split('-')[-1].capitalize() + '; '\n",
    "                            article_aff += given.split(to_find.split('-')[-1].capitalize())[1].strip()[1:] + '; '\n",
    "\n",
    "\n",
    "                        elif len(given.split(to_find.split('-')[-1])) > 1:\n",
    "\n",
    "                            article_author += given.split(to_find.split('-')[-1])[0].strip()[:-1] + ' ' + to_find.split('-')[-1] + '; '\n",
    "                            article_aff += given.split(to_find.split('-')[-1])[1].strip()[1:] + '; '\n",
    "\n",
    "                        elif len(given.split(sten)) > 1:\n",
    "                            article_author += given.split(sten)[0].strip()[:-1] + ' ' + sten + '; '\n",
    "                            article_aff += given.split(sten)[1].strip()[1:] + '; '\n",
    "\n",
    "                if len(article_time.split('-')[0].split()) == 1:\n",
    "                    article_time = article_time.split('-')[0].split()[0] + ' ' + article_time.split('-')[1].split()[1] + ' - ' + article_time.split('-')[1].split()[0] + ' ' + article_time.split('-')[1].split()[1]\n",
    "\n",
    "                print(f'sess title :- {article_title}')\n",
    "                print(f'sess time :- {article_time}')\n",
    "                print(f'sess author :- {article_author}')\n",
    "                print(f'sess aff :- {article_aff}')\n",
    "                print(f'sess text :- {article_text}')\n",
    "                print('=8=8=8='*17)\n",
    "\n",
    "                dic['session_id'].append(session_id)\n",
    "                dic['news_type'].append('Abstract')\n",
    "                dic['source_id'].append('')\n",
    "                dic['manual_id'].append('')\n",
    "                dic['article_title'].append(article_title)\n",
    "                dic['url'].append(abstract_link)\n",
    "                dic['authors'].append(article_author)\n",
    "                dic['author_affiliation'].append(article_aff)\n",
    "                dic['abstract_text'].append(article_text)\n",
    "                dic['date'].append(session_date)\n",
    "                dic['start_time'].append(article_time)\n",
    "                dic['end_time'].append('')\n",
    "                dic['location'].append(session_location)\n",
    "                dic['session_title'].append(session_title)\n",
    "                dic['session_type'].append('')\n",
    "                dic['category'].append('')\n",
    "                dic['sub_category'].append('')\n",
    "                dic['disclosure'].append('')\n",
    "                \n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    c += 1\n",
    "    se_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c5b153",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfdata = pd.DataFrame(dic)\n",
    "dfdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160a11de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdata.to_excel('aasld_dat.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7463b082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881219ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579a4141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3a2685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a0bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessdic = dict()\n",
    "sesstopic = dict()\n",
    "sessprogtype = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0edf34f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# program type filter krke sare link upr ke dict me update \n",
    "\n",
    "flag = True\n",
    "while flag:\n",
    "    \n",
    "    if 'Transplant/Liver Failure' in sesstopic:\n",
    "\n",
    "        print('in if')\n",
    "        time.sleep(5)\n",
    "        home_page = driver.page_source\n",
    "        homesoup = BeautifulSoup(home_page, 'lxml')\n",
    "\n",
    "        for hpg in homesoup.find_all(class_='info-grid__box'):\n",
    "            if hpg.find(class_='info-grid__title').get('href'):\n",
    "                session_link = 'https://www.aasld.org' + hpg.find(class_='info-grid__title').get('href')\n",
    "                print(session_link)\n",
    "                sesstopic['Transplant/Liver Failure'].append(session_link)\n",
    "                \n",
    "        for cl in driver.find_elements(By.TAG_NAME, 'button'):\n",
    "            if cl.text == 'Next' and cl.is_enabled():\n",
    "                cl.click()\n",
    "                time.sleep(6)\n",
    "            elif cl.text == 'Next' and cl.is_enabled() == False:\n",
    "                flag = False\n",
    "\n",
    "\n",
    "    else:\n",
    "        print('in else')\n",
    "        home_page = driver.page_source\n",
    "        homesoup = BeautifulSoup(home_page, 'lxml')\n",
    "\n",
    "        links = []\n",
    "        for hpg in homesoup.find_all(class_='info-grid__box'):\n",
    "            if hpg.find(class_='info-grid__title').get('href'):\n",
    "                session_link = 'https://www.aasld.org' + hpg.find(class_='info-grid__title').get('href')\n",
    "                print(session_link)\n",
    "                links.append(session_link)\n",
    "\n",
    "        sesstopic.update({'Transplant/Liver Failure':links})\n",
    "        \n",
    "        found = False\n",
    "        for cl in driver.find_elements(By.TAG_NAME, 'button'):\n",
    "            if cl.text == 'Next' and cl.is_enabled():\n",
    "                found = True\n",
    "                cl.click()\n",
    "                time.sleep(6)\n",
    "            elif cl.text == 'Next' and cl.is_enabled() == False:\n",
    "                flag = False\n",
    "                \n",
    "        flag = found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358aa3aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sesstopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7311e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump it just in case we get problem with kernal\n",
    "with open('aasld_json_sesstopic.json', 'w') as f:\n",
    "    json.dump(sesstopic, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c84f75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44cebc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urls_links = set(df['urls'])\n",
    "# saare link already hai df urls me to abhi sabko sesstype or category do\n",
    "for i in sesstopic:\n",
    "    for j in sesstopic[i]:\n",
    "        print(j in urls_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3517e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# crawled data me category or sesstype bharo\n",
    "df_data = pd.read_excel('aasld_dat.xlsx')\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86081be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category filling\n",
    "for t in sesstopic:\n",
    "    for u in sesstopic[t]:\n",
    "        # apna data\n",
    "        c = 0\n",
    "        for d in df_data['url']:\n",
    "            if str(u) == str(d):\n",
    "                try:\n",
    "                    df_data.iat[c, df_data.columns.get_loc('category')] += f'; {t}'\n",
    "                except:\n",
    "                    df_data.iat[c, df_data.columns.get_loc('category')] = f'{t}'\n",
    "            c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68e73f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sesstype filling\n",
    "for t in sessdic:\n",
    "    print(t)\n",
    "    for u in sessdic[t]:\n",
    "        # apna data\n",
    "        c = 0\n",
    "        for d in df_data['url']:\n",
    "            if str(u) == str(d):\n",
    "                try:\n",
    "                    df_data.iat[c, df_data.columns.get_loc('session_type')] += f'; {t}'\n",
    "                except:\n",
    "                    df_data.iat[c, df_data.columns.get_loc('session_type')] = f'{t}'\n",
    "            c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84adb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pta nhi filling\n",
    "for t in sessprogtype:\n",
    "    print(t)\n",
    "    for u in sessprogtype[t]:\n",
    "        # apna data\n",
    "        c = 0\n",
    "        for d in df_data['url']:\n",
    "            if str(u) == str(d):\n",
    "                try:\n",
    "                    df_data.iat[c, df_data.columns.get_loc('sub_category')] += f'; {t}'\n",
    "                except:\n",
    "                    df_data.iat[c, df_data.columns.get_loc('sub_category')] = f'{t}'\n",
    "            c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988f4bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final excel with evryshit\n",
    "df_data.to_excel('aasld_final.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf8135a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6fe971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b8e42e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af85241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_sess = []\n",
    "dic = {\n",
    "    'source_id':[],\n",
    "    'manual_id':[],\n",
    "    'article_title':[],\n",
    "    'url':[],\n",
    "    'authors':[],\n",
    "    'author_affiliation':[],\n",
    "    'abstract_text':[],\n",
    "    'date':[],\n",
    "    'start_time':[],\n",
    "    'end_time':[],\n",
    "    'location':[],\n",
    "    'session_title':[],\n",
    "    'session_type':[],\n",
    "    'category':[],\n",
    "    'sub_category':[],\n",
    "    'disclosure':[]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a3a3a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7b98ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c = 0\n",
    "\n",
    "for ou in df_data['url']:\n",
    "    presentations = []\n",
    "    req = requests.get(ou)\n",
    "    soup = BeautifulSoup(req.content, 'lxml')\n",
    "    for i in sibling_cleaner(list(soup.find(class_='col-12 col-lg-7').find('p').next_siblings)):\n",
    "        for pre in i.find_all('a'):\n",
    "            if 'http' not in str(pre.get('href')):\n",
    "                presentations.append(pre.get('href'))\n",
    "\n",
    "    if presentations:\n",
    "        print(ou)\n",
    "        try:\n",
    "        \n",
    "            session_title = df_data.iloc[c]['session_title']\n",
    "            session_location = df_data.iloc[c]['location']\n",
    "            session_type = df_data.iloc[c]['session_type']\n",
    "            session_category = df_data.iloc[c]['category']\n",
    "            sub_cat = df_data.iloc[c]['sub_category']\n",
    "            session_date = df_data.iloc[c]['date']\n",
    "            print(session_title)\n",
    "\n",
    "\n",
    "            for p in presentations:\n",
    "                abstract_link = f'https://www.aasld.org{p}'\n",
    "                print(abstract_link)\n",
    "                print(c)\n",
    "\n",
    "                req = requests.get(abstract_link)\n",
    "                soup = BeautifulSoup(req.content, 'lxml')\n",
    "                article_title = soup.find(class_='col-12 col-lg-7').find('h3').text.strip() # session title\n",
    "\n",
    "                article_time = ''\n",
    "                date_time_room = clean(soup.find(class_='cta-block__content').text)\n",
    "\n",
    "                if re.search('Time:.*', date_time_room, flags=re.S):\n",
    "                    article_time = re.search('Time:(.\\d{1,2}:\\d{1,2}.*- \\d{1,2}:\\d{1,2} [A-Z]{1,2})',date_time_room, flags=re.S).group(1).strip()\n",
    "\n",
    "\n",
    "                # abstract text\n",
    "                article_text = ''\n",
    "                new_text = [soup.find(class_='col-12 col-lg-7').find('p')] + list(soup.find(class_='col-12 col-lg-7').find('p').next_siblings)\n",
    "                for i in sibling_cleaner(new_text):\n",
    "\n",
    "                    ul_text = ''\n",
    "                    if i.name == 'ul':\n",
    "                        for li in i.find_all('li'):\n",
    "                            ul_text += li.text + ' '\n",
    "                        if ul_text:\n",
    "                            article_text += ul_text + ' '\n",
    "\n",
    "                    else:\n",
    "                        article_text += i.text + ' '\n",
    "\n",
    "                # session author\n",
    "                article_author = ''\n",
    "                article_aff = ''\n",
    "                if soup.find(class_='cta-block__content'):\n",
    "                    for i in soup.find(class_='cta-block__content').find_all('a')[:-1]:\n",
    "                        given = i.text.strip()\n",
    "                        to_find = str(i.get('href'))\n",
    "\n",
    "                        sten = to_find.split('-')[-1][0].upper() + to_find.split('-')[-1][1:-1] + to_find.split('-')[-1][-1].upper()\n",
    "                        if len(given.split(to_find.split('-')[-1].upper())) > 1:\n",
    "                            article_author += given.split(to_find.split('-')[-1].upper())[0].strip()[:-1] + ' '+ to_find.split('-')[-1].upper() + '; '\n",
    "                            article_aff += given.split(to_find.split('-')[-1].upper())[1].strip()[1:] + '; '\n",
    "\n",
    "\n",
    "                        elif len(given.split(to_find.split('-')[-1].capitalize())) > 1:\n",
    "\n",
    "                            article_author += given.split(to_find.split('-')[-1].capitalize())[0].strip()[:-1] + ' '+ to_find.split('-')[-1].capitalize() + '; '\n",
    "                            article_aff += given.split(to_find.split('-')[-1].capitalize())[1].strip()[1:] + '; '\n",
    "\n",
    "\n",
    "                        elif len(given.split(to_find.split('-')[-1])) > 1:\n",
    "\n",
    "                            article_author += given.split(to_find.split('-')[-1])[0].strip()[:-1] + ' ' + to_find.split('-')[-1] + '; '\n",
    "                            article_aff += given.split(to_find.split('-')[-1])[1].strip()[1:] + '; '\n",
    "\n",
    "                        elif len(given.split(sten)) > 1:\n",
    "                            article_author += given.split(sten)[0].strip()[:-1] + ' ' + sten + '; '\n",
    "                            article_aff += given.split(sten)[1].strip()[1:] + '; '\n",
    "\n",
    "                if len(article_time.split('-')[0].split()) == 1:\n",
    "                    article_time = article_time.split('-')[0].split()[0] + ' ' + article_time.split('-')[1].split()[1] + ' - ' + article_time.split('-')[1].split()[0] + ' ' + article_time.split('-')[1].split()[1]\n",
    "\n",
    "                print(f'sess title :- {article_title}')\n",
    "                print(f'sess time :- {article_time}')\n",
    "                print(f'sess author :- {article_author}')\n",
    "                print(f'sess aff :- {article_aff}')\n",
    "                print(f'sess text :- {article_text}')\n",
    "                print('=8=8=8='*17)\n",
    "\n",
    "                dic['source_id'].append('')\n",
    "                dic['manual_id'].append('')\n",
    "                dic['article_title'].append(article_title)\n",
    "                dic['url'].append(abstract_link)\n",
    "                dic['authors'].append(article_author)\n",
    "                dic['author_affiliation'].append(article_aff)\n",
    "                dic['abstract_text'].append(article_text)\n",
    "                dic['date'].append(session_date)\n",
    "                dic['start_time'].append(article_time)\n",
    "                dic['end_time'].append('')\n",
    "                dic['location'].append(session_location)\n",
    "                dic['session_title'].append(session_title)\n",
    "                dic['session_type'].append(session_type)\n",
    "                dic['category'].append(session_category)\n",
    "                dic['sub_category'].append(sub_cat)\n",
    "                dic['disclosure'].append('')\n",
    "                \n",
    "                ch_sess.append(ou)\n",
    "        except:\n",
    "            pass\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b80b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_p = pd.DataFrame(dic)\n",
    "df_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4a16f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_excel('aasld_final.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ecb035",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = pd.concat([df_data, df_p])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac44f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_excel('aasld_withpress.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d2f07c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5241883",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
